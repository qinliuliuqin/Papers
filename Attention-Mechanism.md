# This page lists all papers on the topic of attention mechanism

[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) (2017, citation 10181 on 07/12/2020)

This paper proposed the Transformer network to replace RNN and CNN for sequence-to-sequence modeling.
A great tutorial on Transformer network can be found [here](http://jalammar.github.io/illustrated-transformer/)

[End-to-End Multi-Task Learning with Attention](https://arxiv.org/pdf/1803.10704.pdf) (CVPR 2019)

As the name suggests.

